{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c199409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary files and libraries that are required for operationalisation of this Project\n",
    "import os\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a9f6296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to change the gamma of the imagery\n",
    "def adjust_gamma(image):\n",
    "        gamma = 0.5\n",
    "        invGamma = 1.0 / gamma\n",
    "        table = np.array([((i / 255.0) ** invGamma) * 255\n",
    "            for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "\n",
    "        return cv2.LUT(image, table)\n",
    "\n",
    "# Function to change the brightness of the imagery\n",
    "def increase_brightness(img, value):\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    h, s, v = cv2.split(hsv)\n",
    "\n",
    "    lim = 255 - value\n",
    "    v[v > lim] = 255\n",
    "    v[v <= lim] += value\n",
    "\n",
    "    final_hsv = cv2.merge((h, s, v))\n",
    "    img = cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)\n",
    "    return img\n",
    "\n",
    "# Function to train the model\n",
    "def load_train(train_path, image_size, classes):\n",
    "    images = []\n",
    "    labels = []\n",
    "    img_names = []\n",
    "    cls = []\n",
    "\n",
    "    for fields in classes:   \n",
    "        index = classes.index(fields)\n",
    "        path = os.path.join(train_path, fields, '*g')\n",
    "        files = glob.glob(path)\n",
    "        for fl in files:\n",
    "            image = cv2.imread(fl)\n",
    "\n",
    "            # Get only the bottom half of the images, as this is where the road will be\n",
    "            height, width = image.shape[:2]\n",
    "            newHeight = int(round(height/2))\n",
    "            image = image[newHeight-5:height-50, 0:width]\n",
    "            brght_img = increase_brightness(image, value=150)\n",
    "            shaded_img = adjust_gamma(image)\n",
    "            \n",
    "            # Raw image\n",
    "            image = cv2.resize(image, (image_size, image_size),0,0, cv2.INTER_LINEAR)\n",
    "            image = image.astype(np.float32)\n",
    "            image = np.multiply(image, 1.0 / 255.0)\n",
    "\n",
    "            # Make a version of the images that are brightened\n",
    "            brght_img = cv2.resize(brght_img, (image_size, image_size),0,0, cv2.INTER_LINEAR)\n",
    "            brght_img = brght_img.astype(np.float32)\n",
    "            brght_img = np.multiply(brght_img, 1.0 / 255.0)\n",
    "\n",
    "            # Make a version of the images that are shaded\n",
    "            shaded_img = cv2.resize(shaded_img, (image_size, image_size),0,0, cv2.INTER_LINEAR)\n",
    "            shaded_img = shaded_img.astype(np.float32)\n",
    "            shaded_img = np.multiply(shaded_img, 1.0 / 255.0)\n",
    "            \n",
    "            # Add all three images to the dataset for training\n",
    "            images.append(image)\n",
    "            images.append(brght_img)\n",
    "            images.append(shaded_img)\n",
    "\n",
    "            label = np.zeros(len(classes))\n",
    "            label[index] = 1.0\n",
    "\n",
    "            for i in range(3):\n",
    "                labels.append(label)    \n",
    "                flbase = os.path.basename(fl)\n",
    "                img_names.append(flbase)\n",
    "                cls.append(fields)\n",
    "           \n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    img_names = np.array(img_names)\n",
    "    cls = np.array(cls)\n",
    "\n",
    "    return images, labels, img_names, cls\n",
    "\n",
    "# Object oriented coding to let the instance of the dataset reference itself\n",
    "class DataSet(object):\n",
    "\n",
    "  def __init__(self, images, labels, img_names, cls):\n",
    "    self._num_examples = images.shape[0]\n",
    "\n",
    "    self._images = images\n",
    "    self._labels = labels\n",
    "    self._img_names = img_names\n",
    "    self._cls = cls\n",
    "    self._epochs_done = 0\n",
    "    self._index_in_epoch = 0\n",
    "\n",
    "  @property\n",
    "  def images(self):\n",
    "    return self._images\n",
    "\n",
    "  @property\n",
    "  def labels(self):\n",
    "    return self._labels\n",
    "\n",
    "  @property\n",
    "  def img_names(self):\n",
    "    return self._img_names\n",
    "\n",
    "  @property\n",
    "  def cls(self):\n",
    "    return self._cls\n",
    "\n",
    "  @property\n",
    "  def num_examples(self):\n",
    "    return self._num_examples\n",
    "\n",
    "  @property\n",
    "  def epochs_done(self):\n",
    "    return self._epochs_done\n",
    "\n",
    "  def next_batch(self, batch_size):\n",
    "    \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
    "    start = self._index_in_epoch\n",
    "    self._index_in_epoch += batch_size\n",
    "\n",
    "    if self._index_in_epoch > self._num_examples:\n",
    "      self._epochs_done += 1\n",
    "      start = 0\n",
    "      self._index_in_epoch = batch_size\n",
    "      assert batch_size <= self._num_examples\n",
    "    end = self._index_in_epoch\n",
    "\n",
    "    return self._images[start:end], self._labels[start:end], self._img_names[start:end], self._cls[start:end]\n",
    "\n",
    "\n",
    "def read_train_sets(train_path, image_size, classes, validation_size):\n",
    "  class DataSets(object):\n",
    "    pass\n",
    "  data_sets = DataSets()\n",
    "\n",
    "  images, labels, img_names, cls = load_train(train_path, image_size, classes)\n",
    "  images, labels, img_names, cls = shuffle(images, labels, img_names, cls)  \n",
    "\n",
    "  if isinstance(validation_size, float):\n",
    "    validation_size = int(validation_size * images.shape[0])\n",
    "\n",
    "  validation_images = images[:validation_size]\n",
    "  validation_labels = labels[:validation_size]\n",
    "  validation_img_names = img_names[:validation_size]\n",
    "  validation_cls = cls[:validation_size]\n",
    "\n",
    "  train_images = images[validation_size:]\n",
    "  train_labels = labels[validation_size:]\n",
    "  train_img_names = img_names[validation_size:]\n",
    "  train_cls = cls[validation_size:]\n",
    "\n",
    "  data_sets.train = DataSet(train_images, train_labels, train_img_names, train_cls)\n",
    "  data_sets.valid = DataSet(validation_images, validation_labels, validation_img_names, validation_cls)\n",
    "\n",
    "  return data_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15e8cce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to read training images\n",
      "Now going to read 01_tarmac_good files (Index: 0)\n",
      "Now going to read 02_tarmac_regular files (Index: 1)\n",
      "Now going to read 03_tarmac_bad files (Index: 2)\n",
      "Now going to read 04_paved_good files (Index: 3)\n",
      "Now going to read 05_paved_regular files (Index: 4)\n",
      "Now going to read 06_paved_bad files (Index: 5)\n",
      "Now going to read 07_dirt_regular files (Index: 6)\n",
      "Now going to read 08_dirt_bad files (Index: 7)\n",
      "Complete reading input data. Will Now print a snippet of it\n",
      "Number of files in Training-set:\t\t21908\n",
      "Number of files in Validation-set:\t3865\n",
      "WARNING:tensorflow:From C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp/ipykernel_29288/239957495.py:35: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp/ipykernel_29288/239957495.py:36: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp/ipykernel_29288/239957495.py:40: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n",
      "WARNING:tensorflow:From C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp/ipykernel_29288/239957495.py:57: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp/ipykernel_29288/239957495.py:81: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp/ipykernel_29288/239957495.py:153: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp/ipykernel_29288/239957495.py:155: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp/ipykernel_29288/239957495.py:157: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp/ipykernel_29288/239957495.py:173: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "Training Epoch 1 --- Training Accuracy:   3.1%, Validation Accuracy:   6.2%,  Validation Loss: 2.083\n",
      "Training Epoch 2 --- Training Accuracy:  21.9%, Validation Accuracy:  25.0%,  Validation Loss: 1.996\n",
      "Training Epoch 3 --- Training Accuracy:  21.9%, Validation Accuracy:  21.9%,  Validation Loss: 2.037\n",
      "Training Epoch 4 --- Training Accuracy:  21.9%, Validation Accuracy:  25.0%,  Validation Loss: 1.977\n",
      "Training Epoch 5 --- Training Accuracy:  21.9%, Validation Accuracy:  21.9%,  Validation Loss: 1.960\n",
      "Training Epoch 6 --- Training Accuracy:  21.9%, Validation Accuracy:  31.2%,  Validation Loss: 1.904\n",
      "Training Epoch 7 --- Training Accuracy:  21.9%, Validation Accuracy:  18.8%,  Validation Loss: 1.988\n",
      "Training Epoch 8 --- Training Accuracy:  21.9%, Validation Accuracy:  18.8%,  Validation Loss: 2.010\n",
      "Training Epoch 9 --- Training Accuracy:  21.9%, Validation Accuracy:  28.1%,  Validation Loss: 1.899\n",
      "Training Epoch 10 --- Training Accuracy:  21.9%, Validation Accuracy:  21.9%,  Validation Loss: 1.964\n",
      "Training Epoch 11 --- Training Accuracy:  28.1%, Validation Accuracy:  21.9%,  Validation Loss: 1.925\n",
      "Training Epoch 12 --- Training Accuracy:  31.2%, Validation Accuracy:  34.4%,  Validation Loss: 1.809\n",
      "Training Epoch 13 --- Training Accuracy:  31.2%, Validation Accuracy:  25.0%,  Validation Loss: 1.936\n",
      "Training Epoch 14 --- Training Accuracy:  34.4%, Validation Accuracy:  34.4%,  Validation Loss: 1.823\n",
      "Training Epoch 15 --- Training Accuracy:  31.2%, Validation Accuracy:  40.6%,  Validation Loss: 1.778\n",
      "Training Epoch 16 --- Training Accuracy:  34.4%, Validation Accuracy:  34.4%,  Validation Loss: 1.794\n",
      "Training Epoch 17 --- Training Accuracy:  34.4%, Validation Accuracy:  18.8%,  Validation Loss: 1.838\n",
      "Training Epoch 18 --- Training Accuracy:  34.4%, Validation Accuracy:  28.1%,  Validation Loss: 1.821\n",
      "Training Epoch 19 --- Training Accuracy:  34.4%, Validation Accuracy:  34.4%,  Validation Loss: 1.713\n",
      "Training Epoch 20 --- Training Accuracy:  37.5%, Validation Accuracy:  37.5%,  Validation Loss: 1.592\n",
      "Training Epoch 21 --- Training Accuracy:  40.6%, Validation Accuracy:  46.9%,  Validation Loss: 1.561\n",
      "Training Epoch 22 --- Training Accuracy:  43.8%, Validation Accuracy:  46.9%,  Validation Loss: 1.495\n",
      "Training Epoch 23 --- Training Accuracy:  46.9%, Validation Accuracy:  31.2%,  Validation Loss: 1.702\n",
      "Training Epoch 24 --- Training Accuracy:  46.9%, Validation Accuracy:  56.2%,  Validation Loss: 1.485\n",
      "Training Epoch 25 --- Training Accuracy:  50.0%, Validation Accuracy:  53.1%,  Validation Loss: 1.475\n",
      "Training Epoch 26 --- Training Accuracy:  53.1%, Validation Accuracy:  43.8%,  Validation Loss: 1.566\n",
      "Training Epoch 27 --- Training Accuracy:  50.0%, Validation Accuracy:  46.9%,  Validation Loss: 1.511\n",
      "Training Epoch 28 --- Training Accuracy:  50.0%, Validation Accuracy:  46.9%,  Validation Loss: 1.591\n",
      "Training Epoch 29 --- Training Accuracy:  50.0%, Validation Accuracy:  53.1%,  Validation Loss: 1.588\n",
      "Training Epoch 30 --- Training Accuracy:  50.0%, Validation Accuracy:  65.6%,  Validation Loss: 1.209\n",
      "Training Epoch 31 --- Training Accuracy:  50.0%, Validation Accuracy:  56.2%,  Validation Loss: 1.301\n",
      "Training Epoch 32 --- Training Accuracy:  50.0%, Validation Accuracy:  62.5%,  Validation Loss: 1.216\n",
      "Training Epoch 33 --- Training Accuracy:  46.9%, Validation Accuracy:  40.6%,  Validation Loss: 1.529\n",
      "Training Epoch 34 --- Training Accuracy:  46.9%, Validation Accuracy:  65.6%,  Validation Loss: 1.240\n",
      "Training Epoch 35 --- Training Accuracy:  46.9%, Validation Accuracy:  62.5%,  Validation Loss: 1.265\n",
      "Training Epoch 36 --- Training Accuracy:  50.0%, Validation Accuracy:  43.8%,  Validation Loss: 1.385\n",
      "Training Epoch 37 --- Training Accuracy:  50.0%, Validation Accuracy:  56.2%,  Validation Loss: 1.280\n",
      "Training Epoch 38 --- Training Accuracy:  50.0%, Validation Accuracy:  50.0%,  Validation Loss: 1.458\n",
      "Training Epoch 39 --- Training Accuracy:  50.0%, Validation Accuracy:  56.2%,  Validation Loss: 1.519\n",
      "Training Epoch 40 --- Training Accuracy:  50.0%, Validation Accuracy:  68.8%,  Validation Loss: 1.024\n",
      "Training Epoch 41 --- Training Accuracy:  50.0%, Validation Accuracy:  59.4%,  Validation Loss: 1.238\n",
      "Training Epoch 42 --- Training Accuracy:  53.1%, Validation Accuracy:  78.1%,  Validation Loss: 1.046\n",
      "Training Epoch 43 --- Training Accuracy:  53.1%, Validation Accuracy:  50.0%,  Validation Loss: 1.420\n",
      "Training Epoch 44 --- Training Accuracy:  53.1%, Validation Accuracy:  65.6%,  Validation Loss: 1.095\n",
      "Training Epoch 45 --- Training Accuracy:  53.1%, Validation Accuracy:  65.6%,  Validation Loss: 1.151\n",
      "Training Epoch 46 --- Training Accuracy:  53.1%, Validation Accuracy:  46.9%,  Validation Loss: 1.267\n",
      "Training Epoch 47 --- Training Accuracy:  53.1%, Validation Accuracy:  59.4%,  Validation Loss: 1.144\n",
      "Training Epoch 48 --- Training Accuracy:  53.1%, Validation Accuracy:  59.4%,  Validation Loss: 1.358\n",
      "Training Epoch 49 --- Training Accuracy:  53.1%, Validation Accuracy:  59.4%,  Validation Loss: 1.461\n",
      "Training Epoch 50 --- Training Accuracy:  53.1%, Validation Accuracy:  71.9%,  Validation Loss: 0.917\n",
      "Training Epoch 51 --- Training Accuracy:  53.1%, Validation Accuracy:  68.8%,  Validation Loss: 1.207\n",
      "Training Epoch 52 --- Training Accuracy:  53.1%, Validation Accuracy:  87.5%,  Validation Loss: 0.933\n",
      "Training Epoch 53 --- Training Accuracy:  53.1%, Validation Accuracy:  56.2%,  Validation Loss: 1.324\n",
      "Training Epoch 54 --- Training Accuracy:  53.1%, Validation Accuracy:  65.6%,  Validation Loss: 0.984\n",
      "Training Epoch 55 --- Training Accuracy:  53.1%, Validation Accuracy:  65.6%,  Validation Loss: 1.072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 56 --- Training Accuracy:  53.1%, Validation Accuracy:  46.9%,  Validation Loss: 1.181\n",
      "Training Epoch 57 --- Training Accuracy:  53.1%, Validation Accuracy:  62.5%,  Validation Loss: 1.048\n",
      "Training Epoch 58 --- Training Accuracy:  53.1%, Validation Accuracy:  68.8%,  Validation Loss: 1.267\n",
      "Training Epoch 59 --- Training Accuracy:  53.1%, Validation Accuracy:  56.2%,  Validation Loss: 1.405\n",
      "Training Epoch 60 --- Training Accuracy:  53.1%, Validation Accuracy:  68.8%,  Validation Loss: 0.842\n",
      "Training Epoch 61 --- Training Accuracy:  53.1%, Validation Accuracy:  75.0%,  Validation Loss: 1.175\n",
      "Training Epoch 62 --- Training Accuracy:  53.1%, Validation Accuracy:  84.4%,  Validation Loss: 0.852\n",
      "Training Epoch 63 --- Training Accuracy:  53.1%, Validation Accuracy:  56.2%,  Validation Loss: 1.236\n",
      "Training Epoch 64 --- Training Accuracy:  53.1%, Validation Accuracy:  68.8%,  Validation Loss: 0.899\n",
      "Training Epoch 65 --- Training Accuracy:  53.1%, Validation Accuracy:  68.8%,  Validation Loss: 1.001\n",
      "Training Epoch 66 --- Training Accuracy:  53.1%, Validation Accuracy:  56.2%,  Validation Loss: 1.106\n",
      "Training Epoch 67 --- Training Accuracy:  53.1%, Validation Accuracy:  62.5%,  Validation Loss: 0.966\n",
      "Training Epoch 68 --- Training Accuracy:  53.1%, Validation Accuracy:  68.8%,  Validation Loss: 1.173\n",
      "Training Epoch 69 --- Training Accuracy:  53.1%, Validation Accuracy:  56.2%,  Validation Loss: 1.345\n",
      "Training Epoch 70 --- Training Accuracy:  53.1%, Validation Accuracy:  71.9%,  Validation Loss: 0.780\n",
      "Training Epoch 71 --- Training Accuracy:  56.2%, Validation Accuracy:  75.0%,  Validation Loss: 1.131\n",
      "Training Epoch 72 --- Training Accuracy:  56.2%, Validation Accuracy:  87.5%,  Validation Loss: 0.790\n",
      "Training Epoch 73 --- Training Accuracy:  56.2%, Validation Accuracy:  62.5%,  Validation Loss: 1.148\n",
      "Training Epoch 74 --- Training Accuracy:  56.2%, Validation Accuracy:  68.8%,  Validation Loss: 0.826\n",
      "Training Epoch 75 --- Training Accuracy:  56.2%, Validation Accuracy:  71.9%,  Validation Loss: 0.936\n",
      "Training Epoch 76 --- Training Accuracy:  56.2%, Validation Accuracy:  56.2%,  Validation Loss: 1.036\n",
      "Training Epoch 77 --- Training Accuracy:  56.2%, Validation Accuracy:  62.5%,  Validation Loss: 0.899\n",
      "Training Epoch 78 --- Training Accuracy:  56.2%, Validation Accuracy:  75.0%,  Validation Loss: 1.083\n",
      "Training Epoch 79 --- Training Accuracy:  56.2%, Validation Accuracy:  56.2%,  Validation Loss: 1.285\n",
      "Training Epoch 80 --- Training Accuracy:  56.2%, Validation Accuracy:  75.0%,  Validation Loss: 0.723\n",
      "Training Epoch 81 --- Training Accuracy:  56.2%, Validation Accuracy:  75.0%,  Validation Loss: 1.079\n",
      "Training Epoch 82 --- Training Accuracy:  59.4%, Validation Accuracy:  84.4%,  Validation Loss: 0.740\n",
      "Training Epoch 83 --- Training Accuracy:  59.4%, Validation Accuracy:  65.6%,  Validation Loss: 1.076\n",
      "Training Epoch 84 --- Training Accuracy:  59.4%, Validation Accuracy:  68.8%,  Validation Loss: 0.765\n",
      "Training Epoch 85 --- Training Accuracy:  59.4%, Validation Accuracy:  78.1%,  Validation Loss: 0.879\n",
      "Training Epoch 86 --- Training Accuracy:  59.4%, Validation Accuracy:  56.2%,  Validation Loss: 0.970\n",
      "Training Epoch 87 --- Training Accuracy:  59.4%, Validation Accuracy:  62.5%,  Validation Loss: 0.839\n",
      "Training Epoch 88 --- Training Accuracy:  59.4%, Validation Accuracy:  81.2%,  Validation Loss: 1.000\n",
      "Training Epoch 89 --- Training Accuracy:  59.4%, Validation Accuracy:  59.4%,  Validation Loss: 1.227\n",
      "Training Epoch 90 --- Training Accuracy:  59.4%, Validation Accuracy:  71.9%,  Validation Loss: 0.679\n",
      "Training Epoch 91 --- Training Accuracy:  59.4%, Validation Accuracy:  75.0%,  Validation Loss: 1.020\n",
      "Training Epoch 92 --- Training Accuracy:  62.5%, Validation Accuracy:  81.2%,  Validation Loss: 0.699\n",
      "Training Epoch 93 --- Training Accuracy:  62.5%, Validation Accuracy:  75.0%,  Validation Loss: 1.010\n",
      "Training Epoch 94 --- Training Accuracy:  62.5%, Validation Accuracy:  68.8%,  Validation Loss: 0.710\n",
      "Training Epoch 95 --- Training Accuracy:  62.5%, Validation Accuracy:  78.1%,  Validation Loss: 0.823\n",
      "Training Epoch 96 --- Training Accuracy:  62.5%, Validation Accuracy:  59.4%,  Validation Loss: 0.911\n",
      "Training Epoch 97 --- Training Accuracy:  62.5%, Validation Accuracy:  65.6%,  Validation Loss: 0.783\n",
      "Training Epoch 98 --- Training Accuracy:  65.6%, Validation Accuracy:  81.2%,  Validation Loss: 0.922\n",
      "Training Epoch 99 --- Training Accuracy:  65.6%, Validation Accuracy:  59.4%,  Validation Loss: 1.170\n",
      "Training Epoch 100 --- Training Accuracy:  65.6%, Validation Accuracy:  75.0%,  Validation Loss: 0.641\n",
      "Training Epoch 101 --- Training Accuracy:  65.6%, Validation Accuracy:  75.0%,  Validation Loss: 0.958\n",
      "Training Epoch 102 --- Training Accuracy:  65.6%, Validation Accuracy:  81.2%,  Validation Loss: 0.667\n",
      "Training Epoch 103 --- Training Accuracy:  65.6%, Validation Accuracy:  75.0%,  Validation Loss: 0.950\n",
      "Training Epoch 104 --- Training Accuracy:  65.6%, Validation Accuracy:  78.1%,  Validation Loss: 0.659\n",
      "Training Epoch 105 --- Training Accuracy:  68.8%, Validation Accuracy:  78.1%,  Validation Loss: 0.769\n",
      "Training Epoch 106 --- Training Accuracy:  68.8%, Validation Accuracy:  65.6%,  Validation Loss: 0.852\n",
      "Training Epoch 107 --- Training Accuracy:  68.8%, Validation Accuracy:  75.0%,  Validation Loss: 0.733\n",
      "Training Epoch 108 --- Training Accuracy:  68.8%, Validation Accuracy:  81.2%,  Validation Loss: 0.852\n",
      "Training Epoch 109 --- Training Accuracy:  71.9%, Validation Accuracy:  59.4%,  Validation Loss: 1.115\n",
      "Training Epoch 110 --- Training Accuracy:  71.9%, Validation Accuracy:  78.1%,  Validation Loss: 0.604\n",
      "Training Epoch 111 --- Training Accuracy:  71.9%, Validation Accuracy:  78.1%,  Validation Loss: 0.894\n",
      "Training Epoch 112 --- Training Accuracy:  71.9%, Validation Accuracy:  84.4%,  Validation Loss: 0.646\n",
      "Training Epoch 113 --- Training Accuracy:  71.9%, Validation Accuracy:  78.1%,  Validation Loss: 0.898\n",
      "Training Epoch 114 --- Training Accuracy:  71.9%, Validation Accuracy:  84.4%,  Validation Loss: 0.613\n",
      "Training Epoch 115 --- Training Accuracy:  71.9%, Validation Accuracy:  78.1%,  Validation Loss: 0.716\n",
      "Training Epoch 116 --- Training Accuracy:  71.9%, Validation Accuracy:  68.8%,  Validation Loss: 0.798\n",
      "Training Epoch 117 --- Training Accuracy:  71.9%, Validation Accuracy:  78.1%,  Validation Loss: 0.688\n",
      "Training Epoch 118 --- Training Accuracy:  71.9%, Validation Accuracy:  84.4%,  Validation Loss: 0.793\n",
      "Training Epoch 119 --- Training Accuracy:  71.9%, Validation Accuracy:  62.5%,  Validation Loss: 1.058\n",
      "Training Epoch 120 --- Training Accuracy:  71.9%, Validation Accuracy:  78.1%,  Validation Loss: 0.565\n",
      "Training Epoch 121 --- Training Accuracy:  75.0%, Validation Accuracy:  78.1%,  Validation Loss: 0.830\n",
      "Training Epoch 122 --- Training Accuracy:  75.0%, Validation Accuracy:  84.4%,  Validation Loss: 0.627\n",
      "Training Epoch 123 --- Training Accuracy:  75.0%, Validation Accuracy:  78.1%,  Validation Loss: 0.848\n",
      "Training Epoch 124 --- Training Accuracy:  75.0%, Validation Accuracy:  84.4%,  Validation Loss: 0.572\n",
      "Training Epoch 125 --- Training Accuracy:  75.0%, Validation Accuracy:  81.2%,  Validation Loss: 0.663\n",
      "Training Epoch 126 --- Training Accuracy:  75.0%, Validation Accuracy:  71.9%,  Validation Loss: 0.745\n",
      "Training Epoch 127 --- Training Accuracy:  75.0%, Validation Accuracy:  81.2%,  Validation Loss: 0.650\n",
      "Training Epoch 128 --- Training Accuracy:  75.0%, Validation Accuracy:  84.4%,  Validation Loss: 0.737\n",
      "Training Epoch 129 --- Training Accuracy:  75.0%, Validation Accuracy:  62.5%,  Validation Loss: 1.005\n",
      "Training Epoch 130 --- Training Accuracy:  75.0%, Validation Accuracy:  78.1%,  Validation Loss: 0.531\n",
      "Training Epoch 131 --- Training Accuracy:  75.0%, Validation Accuracy:  78.1%,  Validation Loss: 0.772\n",
      "Training Epoch 132 --- Training Accuracy:  75.0%, Validation Accuracy:  84.4%,  Validation Loss: 0.607\n",
      "Training Epoch 133 --- Training Accuracy:  75.0%, Validation Accuracy:  78.1%,  Validation Loss: 0.803\n",
      "Training Epoch 134 --- Training Accuracy:  75.0%, Validation Accuracy:  84.4%,  Validation Loss: 0.532\n",
      "Training Epoch 135 --- Training Accuracy:  75.0%, Validation Accuracy:  87.5%,  Validation Loss: 0.608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 136 --- Training Accuracy:  75.0%, Validation Accuracy:  71.9%,  Validation Loss: 0.698\n",
      "Training Epoch 137 --- Training Accuracy:  75.0%, Validation Accuracy:  81.2%,  Validation Loss: 0.617\n",
      "Training Epoch 138 --- Training Accuracy:  75.0%, Validation Accuracy:  84.4%,  Validation Loss: 0.691\n",
      "Training Epoch 139 --- Training Accuracy:  75.0%, Validation Accuracy:  59.4%,  Validation Loss: 0.954\n",
      "Training Epoch 140 --- Training Accuracy:  75.0%, Validation Accuracy:  78.1%,  Validation Loss: 0.498\n",
      "Training Epoch 141 --- Training Accuracy:  75.0%, Validation Accuracy:  78.1%,  Validation Loss: 0.724\n",
      "Training Epoch 142 --- Training Accuracy:  75.0%, Validation Accuracy:  84.4%,  Validation Loss: 0.591\n",
      "Training Epoch 143 --- Training Accuracy:  75.0%, Validation Accuracy:  78.1%,  Validation Loss: 0.760\n",
      "Training Epoch 144 --- Training Accuracy:  75.0%, Validation Accuracy:  84.4%,  Validation Loss: 0.499\n",
      "Training Epoch 145 --- Training Accuracy:  75.0%, Validation Accuracy:  87.5%,  Validation Loss: 0.557\n",
      "Training Epoch 146 --- Training Accuracy:  75.0%, Validation Accuracy:  78.1%,  Validation Loss: 0.655\n",
      "Training Epoch 147 --- Training Accuracy:  75.0%, Validation Accuracy:  81.2%,  Validation Loss: 0.586\n",
      "Training Epoch 148 --- Training Accuracy:  75.0%, Validation Accuracy:  87.5%,  Validation Loss: 0.652\n",
      "Training Epoch 149 --- Training Accuracy:  75.0%, Validation Accuracy:  62.5%,  Validation Loss: 0.902\n",
      "Training Epoch 150 --- Training Accuracy:  75.0%, Validation Accuracy:  81.2%,  Validation Loss: 0.467\n",
      "Training Epoch 151 --- Training Accuracy:  75.0%, Validation Accuracy:  78.1%,  Validation Loss: 0.682\n",
      "Training Epoch 152 --- Training Accuracy:  75.0%, Validation Accuracy:  84.4%,  Validation Loss: 0.573\n",
      "Training Epoch 153 --- Training Accuracy:  75.0%, Validation Accuracy:  81.2%,  Validation Loss: 0.715\n",
      "Training Epoch 154 --- Training Accuracy:  75.0%, Validation Accuracy:  87.5%,  Validation Loss: 0.467\n",
      "Training Epoch 155 --- Training Accuracy:  75.0%, Validation Accuracy:  84.4%,  Validation Loss: 0.508\n",
      "Training Epoch 156 --- Training Accuracy:  75.0%, Validation Accuracy:  81.2%,  Validation Loss: 0.614\n",
      "Training Epoch 157 --- Training Accuracy:  75.0%, Validation Accuracy:  81.2%,  Validation Loss: 0.557\n",
      "Training Epoch 158 --- Training Accuracy:  75.0%, Validation Accuracy:  87.5%,  Validation Loss: 0.616\n",
      "Training Epoch 159 --- Training Accuracy:  75.0%, Validation Accuracy:  62.5%,  Validation Loss: 0.853\n",
      "Training Epoch 160 --- Training Accuracy:  78.1%, Validation Accuracy:  81.2%,  Validation Loss: 0.434\n",
      "Training Epoch 161 --- Training Accuracy:  78.1%, Validation Accuracy:  81.2%,  Validation Loss: 0.645\n",
      "Training Epoch 162 --- Training Accuracy:  78.1%, Validation Accuracy:  84.4%,  Validation Loss: 0.553\n",
      "Training Epoch 163 --- Training Accuracy:  78.1%, Validation Accuracy:  84.4%,  Validation Loss: 0.677\n",
      "Training Epoch 164 --- Training Accuracy:  78.1%, Validation Accuracy:  87.5%,  Validation Loss: 0.441\n",
      "Training Epoch 165 --- Training Accuracy:  78.1%, Validation Accuracy:  87.5%,  Validation Loss: 0.463\n",
      "Training Epoch 166 --- Training Accuracy:  78.1%, Validation Accuracy:  84.4%,  Validation Loss: 0.579\n",
      "Training Epoch 167 --- Training Accuracy:  81.2%, Validation Accuracy:  81.2%,  Validation Loss: 0.532\n",
      "Training Epoch 168 --- Training Accuracy:  81.2%, Validation Accuracy:  87.5%,  Validation Loss: 0.584\n",
      "Training Epoch 169 --- Training Accuracy:  81.2%, Validation Accuracy:  68.8%,  Validation Loss: 0.807\n",
      "Training Epoch 170 --- Training Accuracy:  81.2%, Validation Accuracy:  84.4%,  Validation Loss: 0.405\n",
      "Training Epoch 171 --- Training Accuracy:  81.2%, Validation Accuracy:  81.2%,  Validation Loss: 0.613\n",
      "Training Epoch 172 --- Training Accuracy:  81.2%, Validation Accuracy:  84.4%,  Validation Loss: 0.529\n",
      "Training Epoch 173 --- Training Accuracy:  81.2%, Validation Accuracy:  84.4%,  Validation Loss: 0.640\n",
      "Training Epoch 174 --- Training Accuracy:  84.4%, Validation Accuracy:  87.5%,  Validation Loss: 0.415\n",
      "Training Epoch 175 --- Training Accuracy:  84.4%, Validation Accuracy:  90.6%,  Validation Loss: 0.424\n",
      "Training Epoch 176 --- Training Accuracy:  84.4%, Validation Accuracy:  84.4%,  Validation Loss: 0.548\n",
      "Training Epoch 177 --- Training Accuracy:  84.4%, Validation Accuracy:  81.2%,  Validation Loss: 0.508\n",
      "Training Epoch 178 --- Training Accuracy:  84.4%, Validation Accuracy:  87.5%,  Validation Loss: 0.557\n",
      "Training Epoch 179 --- Training Accuracy:  84.4%, Validation Accuracy:  68.8%,  Validation Loss: 0.764\n",
      "Training Epoch 180 --- Training Accuracy:  84.4%, Validation Accuracy:  87.5%,  Validation Loss: 0.377\n",
      "Training Epoch 181 --- Training Accuracy:  84.4%, Validation Accuracy:  81.2%,  Validation Loss: 0.584\n",
      "Training Epoch 182 --- Training Accuracy:  84.4%, Validation Accuracy:  87.5%,  Validation Loss: 0.503\n",
      "Training Epoch 183 --- Training Accuracy:  84.4%, Validation Accuracy:  84.4%,  Validation Loss: 0.607\n",
      "Training Epoch 184 --- Training Accuracy:  84.4%, Validation Accuracy:  90.6%,  Validation Loss: 0.393\n",
      "Training Epoch 185 --- Training Accuracy:  84.4%, Validation Accuracy:  93.8%,  Validation Loss: 0.390\n",
      "Training Epoch 186 --- Training Accuracy:  84.4%, Validation Accuracy:  87.5%,  Validation Loss: 0.520\n",
      "Training Epoch 187 --- Training Accuracy:  84.4%, Validation Accuracy:  81.2%,  Validation Loss: 0.487\n",
      "Training Epoch 188 --- Training Accuracy:  84.4%, Validation Accuracy:  87.5%,  Validation Loss: 0.530\n",
      "Training Epoch 189 --- Training Accuracy:  84.4%, Validation Accuracy:  71.9%,  Validation Loss: 0.723\n",
      "Training Epoch 190 --- Training Accuracy:  84.4%, Validation Accuracy:  87.5%,  Validation Loss: 0.353\n",
      "Training Epoch 191 --- Training Accuracy:  84.4%, Validation Accuracy:  81.2%,  Validation Loss: 0.560\n",
      "Training Epoch 192 --- Training Accuracy:  84.4%, Validation Accuracy:  90.6%,  Validation Loss: 0.475\n",
      "Training Epoch 193 --- Training Accuracy:  84.4%, Validation Accuracy:  87.5%,  Validation Loss: 0.576\n",
      "Training Epoch 194 --- Training Accuracy:  84.4%, Validation Accuracy:  90.6%,  Validation Loss: 0.371\n",
      "Training Epoch 195 --- Training Accuracy:  84.4%, Validation Accuracy:  93.8%,  Validation Loss: 0.361\n",
      "Training Epoch 196 --- Training Accuracy:  84.4%, Validation Accuracy:  90.6%,  Validation Loss: 0.499\n",
      "Training Epoch 197 --- Training Accuracy:  84.4%, Validation Accuracy:  84.4%,  Validation Loss: 0.469\n",
      "Training Epoch 198 --- Training Accuracy:  84.4%, Validation Accuracy:  87.5%,  Validation Loss: 0.504\n",
      "Training Epoch 199 --- Training Accuracy:  84.4%, Validation Accuracy:  71.9%,  Validation Loss: 0.682\n",
      "Training Epoch 200 --- Training Accuracy:  84.4%, Validation Accuracy:  87.5%,  Validation Loss: 0.331\n",
      "Training Epoch 201 --- Training Accuracy:  84.4%, Validation Accuracy:  87.5%,  Validation Loss: 0.536\n",
      "Training Epoch 202 --- Training Accuracy:  84.4%, Validation Accuracy:  90.6%,  Validation Loss: 0.446\n",
      "Training Epoch 203 --- Training Accuracy:  84.4%, Validation Accuracy:  87.5%,  Validation Loss: 0.552\n",
      "Training Epoch 204 --- Training Accuracy:  84.4%, Validation Accuracy:  90.6%,  Validation Loss: 0.350\n",
      "Training Epoch 205 --- Training Accuracy:  84.4%, Validation Accuracy:  93.8%,  Validation Loss: 0.335\n",
      "Training Epoch 206 --- Training Accuracy:  84.4%, Validation Accuracy:  90.6%,  Validation Loss: 0.479\n",
      "Training Epoch 207 --- Training Accuracy:  84.4%, Validation Accuracy:  84.4%,  Validation Loss: 0.450\n",
      "Training Epoch 208 --- Training Accuracy:  84.4%, Validation Accuracy:  87.5%,  Validation Loss: 0.483\n",
      "Training Epoch 209 --- Training Accuracy:  84.4%, Validation Accuracy:  75.0%,  Validation Loss: 0.643\n",
      "Training Epoch 210 --- Training Accuracy:  84.4%, Validation Accuracy:  87.5%,  Validation Loss: 0.310\n",
      "Training Epoch 211 --- Training Accuracy:  84.4%, Validation Accuracy:  87.5%,  Validation Loss: 0.515\n",
      "Training Epoch 212 --- Training Accuracy:  84.4%, Validation Accuracy:  90.6%,  Validation Loss: 0.415\n",
      "Training Epoch 213 --- Training Accuracy:  84.4%, Validation Accuracy:  84.4%,  Validation Loss: 0.527\n",
      "Training Epoch 214 --- Training Accuracy:  84.4%, Validation Accuracy:  93.8%,  Validation Loss: 0.331\n",
      "Training Epoch 215 --- Training Accuracy:  84.4%, Validation Accuracy:  93.8%,  Validation Loss: 0.315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 216 --- Training Accuracy:  84.4%, Validation Accuracy:  90.6%,  Validation Loss: 0.461\n",
      "Training Epoch 217 --- Training Accuracy:  84.4%, Validation Accuracy:  84.4%,  Validation Loss: 0.432\n",
      "Training Epoch 218 --- Training Accuracy:  84.4%, Validation Accuracy:  87.5%,  Validation Loss: 0.461\n",
      "Training Epoch 219 --- Training Accuracy:  84.4%, Validation Accuracy:  75.0%,  Validation Loss: 0.608\n",
      "Training Epoch 220 --- Training Accuracy:  84.4%, Validation Accuracy:  90.6%,  Validation Loss: 0.290\n",
      "Training Epoch 221 --- Training Accuracy:  84.4%, Validation Accuracy:  90.6%,  Validation Loss: 0.495\n",
      "Training Epoch 222 --- Training Accuracy:  84.4%, Validation Accuracy:  90.6%,  Validation Loss: 0.383\n",
      "Training Epoch 223 --- Training Accuracy:  84.4%, Validation Accuracy:  87.5%,  Validation Loss: 0.505\n",
      "Training Epoch 224 --- Training Accuracy:  84.4%, Validation Accuracy:  93.8%,  Validation Loss: 0.314\n",
      "Training Epoch 225 --- Training Accuracy:  84.4%, Validation Accuracy:  93.8%,  Validation Loss: 0.297\n",
      "Training Epoch 226 --- Training Accuracy:  84.4%, Validation Accuracy:  90.6%,  Validation Loss: 0.445\n",
      "Training Epoch 227 --- Training Accuracy:  84.4%, Validation Accuracy:  84.4%,  Validation Loss: 0.416\n",
      "Training Epoch 228 --- Training Accuracy:  84.4%, Validation Accuracy:  87.5%,  Validation Loss: 0.441\n",
      "Training Epoch 229 --- Training Accuracy:  84.4%, Validation Accuracy:  75.0%,  Validation Loss: 0.574\n",
      "Training Epoch 230 --- Training Accuracy:  84.4%, Validation Accuracy:  90.6%,  Validation Loss: 0.271\n",
      "Training Epoch 231 --- Training Accuracy:  84.4%, Validation Accuracy:  90.6%,  Validation Loss: 0.475\n",
      "Training Epoch 232 --- Training Accuracy:  84.4%, Validation Accuracy:  90.6%,  Validation Loss: 0.353\n",
      "Training Epoch 233 --- Training Accuracy:  84.4%, Validation Accuracy:  90.6%,  Validation Loss: 0.484\n",
      "Training Epoch 234 --- Training Accuracy:  84.4%, Validation Accuracy:  93.8%,  Validation Loss: 0.297\n",
      "Training Epoch 235 --- Training Accuracy:  84.4%, Validation Accuracy:  93.8%,  Validation Loss: 0.282\n",
      "Training Epoch 236 --- Training Accuracy:  84.4%, Validation Accuracy:  90.6%,  Validation Loss: 0.431\n",
      "Training Epoch 237 --- Training Accuracy:  84.4%, Validation Accuracy:  84.4%,  Validation Loss: 0.400\n",
      "Training Epoch 238 --- Training Accuracy:  84.4%, Validation Accuracy:  87.5%,  Validation Loss: 0.421\n",
      "Training Epoch 239 --- Training Accuracy:  84.4%, Validation Accuracy:  78.1%,  Validation Loss: 0.543\n",
      "Training Epoch 240 --- Training Accuracy:  84.4%, Validation Accuracy:  90.6%,  Validation Loss: 0.252\n",
      "Training Epoch 241 --- Training Accuracy:  84.4%, Validation Accuracy:  93.8%,  Validation Loss: 0.456\n",
      "Training Epoch 242 --- Training Accuracy:  84.4%, Validation Accuracy:  90.6%,  Validation Loss: 0.324\n",
      "Training Epoch 243 --- Training Accuracy:  84.4%, Validation Accuracy:  93.8%,  Validation Loss: 0.465\n",
      "Training Epoch 244 --- Training Accuracy:  84.4%, Validation Accuracy:  93.8%,  Validation Loss: 0.280\n",
      "Training Epoch 245 --- Training Accuracy:  84.4%, Validation Accuracy:  93.8%,  Validation Loss: 0.269\n",
      "Training Epoch 246 --- Training Accuracy:  84.4%, Validation Accuracy:  90.6%,  Validation Loss: 0.419\n",
      "Training Epoch 247 --- Training Accuracy:  84.4%, Validation Accuracy:  84.4%,  Validation Loss: 0.383\n",
      "Training Epoch 248 --- Training Accuracy:  84.4%, Validation Accuracy:  90.6%,  Validation Loss: 0.401\n",
      "Training Epoch 249 --- Training Accuracy:  84.4%, Validation Accuracy:  78.1%,  Validation Loss: 0.514\n",
      "Training Epoch 250 --- Training Accuracy:  84.4%, Validation Accuracy:  90.6%,  Validation Loss: 0.235\n",
      "Training Epoch 251 --- Training Accuracy:  84.4%, Validation Accuracy:  93.8%,  Validation Loss: 0.438\n",
      "Training Epoch 252 --- Training Accuracy:  84.4%, Validation Accuracy:  90.6%,  Validation Loss: 0.297\n",
      "Training Epoch 253 --- Training Accuracy:  84.4%, Validation Accuracy:  93.8%,  Validation Loss: 0.448\n",
      "Training Epoch 254 --- Training Accuracy:  84.4%, Validation Accuracy:  93.8%,  Validation Loss: 0.264\n",
      "Training Epoch 255 --- Training Accuracy:  84.4%, Validation Accuracy:  93.8%,  Validation Loss: 0.258\n",
      "Training Epoch 256 --- Training Accuracy:  84.4%, Validation Accuracy:  90.6%,  Validation Loss: 0.407\n",
      "Training Epoch 257 --- Training Accuracy:  84.4%, Validation Accuracy:  84.4%,  Validation Loss: 0.367\n",
      "Training Epoch 258 --- Training Accuracy:  84.4%, Validation Accuracy:  90.6%,  Validation Loss: 0.383\n",
      "Training Epoch 259 --- Training Accuracy:  84.4%, Validation Accuracy:  78.1%,  Validation Loss: 0.486\n",
      "Training Epoch 260 --- Training Accuracy:  84.4%, Validation Accuracy:  93.8%,  Validation Loss: 0.217\n",
      "Training Epoch 261 --- Training Accuracy:  84.4%, Validation Accuracy:  93.8%,  Validation Loss: 0.423\n",
      "Training Epoch 262 --- Training Accuracy:  84.4%, Validation Accuracy:  90.6%,  Validation Loss: 0.273\n",
      "Training Epoch 263 --- Training Accuracy:  84.4%, Validation Accuracy:  93.8%,  Validation Loss: 0.434\n",
      "Training Epoch 264 --- Training Accuracy:  84.4%, Validation Accuracy:  93.8%,  Validation Loss: 0.249\n",
      "Training Epoch 265 --- Training Accuracy:  84.4%, Validation Accuracy:  93.8%,  Validation Loss: 0.248\n",
      "Training Epoch 266 --- Training Accuracy:  84.4%, Validation Accuracy:  90.6%,  Validation Loss: 0.396\n",
      "Training Epoch 267 --- Training Accuracy:  84.4%, Validation Accuracy:  84.4%,  Validation Loss: 0.352\n",
      "Training Epoch 268 --- Training Accuracy:  84.4%, Validation Accuracy:  93.8%,  Validation Loss: 0.365\n",
      "Training Epoch 269 --- Training Accuracy:  84.4%, Validation Accuracy:  81.2%,  Validation Loss: 0.461\n",
      "Training Epoch 270 --- Training Accuracy:  87.5%, Validation Accuracy:  96.9%,  Validation Loss: 0.202\n",
      "Training Epoch 271 --- Training Accuracy:  87.5%, Validation Accuracy:  96.9%,  Validation Loss: 0.410\n",
      "Training Epoch 272 --- Training Accuracy:  87.5%, Validation Accuracy:  93.8%,  Validation Loss: 0.251\n",
      "Training Epoch 273 --- Training Accuracy:  87.5%, Validation Accuracy:  93.8%,  Validation Loss: 0.422\n",
      "Training Epoch 274 --- Training Accuracy:  87.5%, Validation Accuracy:  93.8%,  Validation Loss: 0.235\n",
      "Training Epoch 275 --- Training Accuracy:  87.5%, Validation Accuracy:  93.8%,  Validation Loss: 0.240\n",
      "Training Epoch 276 --- Training Accuracy:  87.5%, Validation Accuracy:  90.6%,  Validation Loss: 0.387\n",
      "Training Epoch 277 --- Training Accuracy:  87.5%, Validation Accuracy:  87.5%,  Validation Loss: 0.336\n",
      "Training Epoch 278 --- Training Accuracy:  87.5%, Validation Accuracy:  93.8%,  Validation Loss: 0.348\n",
      "Training Epoch 279 --- Training Accuracy:  87.5%, Validation Accuracy:  81.2%,  Validation Loss: 0.437\n",
      "Training Epoch 280 --- Training Accuracy:  87.5%, Validation Accuracy:  96.9%,  Validation Loss: 0.187\n",
      "Training Epoch 281 --- Training Accuracy:  87.5%, Validation Accuracy:  96.9%,  Validation Loss: 0.398\n",
      "Training Epoch 282 --- Training Accuracy:  87.5%, Validation Accuracy:  93.8%,  Validation Loss: 0.230\n",
      "Training Epoch 283 --- Training Accuracy:  87.5%, Validation Accuracy:  93.8%,  Validation Loss: 0.411\n",
      "Training Epoch 284 --- Training Accuracy:  87.5%, Validation Accuracy:  96.9%,  Validation Loss: 0.223\n",
      "Training Epoch 285 --- Training Accuracy:  87.5%, Validation Accuracy:  93.8%,  Validation Loss: 0.232\n",
      "Training Epoch 286 --- Training Accuracy:  87.5%, Validation Accuracy:  90.6%,  Validation Loss: 0.377\n",
      "Training Epoch 287 --- Training Accuracy:  87.5%, Validation Accuracy:  87.5%,  Validation Loss: 0.321\n",
      "Training Epoch 288 --- Training Accuracy:  87.5%, Validation Accuracy:  93.8%,  Validation Loss: 0.332\n",
      "Training Epoch 289 --- Training Accuracy:  87.5%, Validation Accuracy:  81.2%,  Validation Loss: 0.416\n",
      "Training Epoch 290 --- Training Accuracy:  87.5%, Validation Accuracy:  96.9%,  Validation Loss: 0.173\n",
      "Training Epoch 291 --- Training Accuracy:  87.5%, Validation Accuracy:  96.9%,  Validation Loss: 0.388\n",
      "Training Epoch 292 --- Training Accuracy:  87.5%, Validation Accuracy:  96.9%,  Validation Loss: 0.210\n",
      "Training Epoch 293 --- Training Accuracy:  87.5%, Validation Accuracy:  93.8%,  Validation Loss: 0.401\n",
      "Training Epoch 294 --- Training Accuracy:  87.5%, Validation Accuracy:  96.9%,  Validation Loss: 0.212\n",
      "Training Epoch 295 --- Training Accuracy:  87.5%, Validation Accuracy:  93.8%,  Validation Loss: 0.224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 296 --- Training Accuracy:  87.5%, Validation Accuracy:  90.6%,  Validation Loss: 0.368\n",
      "Training Epoch 297 --- Training Accuracy:  87.5%, Validation Accuracy:  87.5%,  Validation Loss: 0.305\n",
      "Training Epoch 298 --- Training Accuracy:  87.5%, Validation Accuracy:  93.8%,  Validation Loss: 0.317\n",
      "Training Epoch 299 --- Training Accuracy:  87.5%, Validation Accuracy:  81.2%,  Validation Loss: 0.397\n",
      "Training Epoch 300 --- Training Accuracy:  87.5%, Validation Accuracy:  96.9%,  Validation Loss: 0.160\n",
      "Training Epoch 301 --- Training Accuracy:  87.5%, Validation Accuracy:  96.9%,  Validation Loss: 0.379\n",
      "Training Epoch 302 --- Training Accuracy:  87.5%, Validation Accuracy:  96.9%,  Validation Loss: 0.193\n",
      "Training Epoch 303 --- Training Accuracy:  87.5%, Validation Accuracy:  93.8%,  Validation Loss: 0.392\n",
      "Training Epoch 304 --- Training Accuracy:  87.5%, Validation Accuracy:  96.9%,  Validation Loss: 0.200\n",
      "Training Epoch 305 --- Training Accuracy:  87.5%, Validation Accuracy:  93.8%,  Validation Loss: 0.218\n",
      "Training Epoch 306 --- Training Accuracy:  87.5%, Validation Accuracy:  90.6%,  Validation Loss: 0.361\n",
      "Training Epoch 307 --- Training Accuracy:  87.5%, Validation Accuracy:  87.5%,  Validation Loss: 0.290\n",
      "Training Epoch 308 --- Training Accuracy:  87.5%, Validation Accuracy:  93.8%,  Validation Loss: 0.302\n",
      "Training Epoch 309 --- Training Accuracy:  87.5%, Validation Accuracy:  84.4%,  Validation Loss: 0.379\n",
      "Training Epoch 310 --- Training Accuracy:  87.5%, Validation Accuracy:  96.9%,  Validation Loss: 0.147\n",
      "Training Epoch 311 --- Training Accuracy:  87.5%, Validation Accuracy:  96.9%,  Validation Loss: 0.369\n",
      "Training Epoch 312 --- Training Accuracy:  87.5%, Validation Accuracy:  96.9%,  Validation Loss: 0.179\n",
      "Training Epoch 313 --- Training Accuracy:  87.5%, Validation Accuracy:  93.8%,  Validation Loss: 0.384\n",
      "Training Epoch 314 --- Training Accuracy:  87.5%, Validation Accuracy:  96.9%,  Validation Loss: 0.188\n",
      "Training Epoch 315 --- Training Accuracy:  87.5%, Validation Accuracy:  93.8%,  Validation Loss: 0.212\n",
      "Training Epoch 316 --- Training Accuracy:  87.5%, Validation Accuracy:  90.6%,  Validation Loss: 0.353\n",
      "Training Epoch 317 --- Training Accuracy:  87.5%, Validation Accuracy:  87.5%,  Validation Loss: 0.274\n",
      "Training Epoch 318 --- Training Accuracy:  90.6%, Validation Accuracy:  93.8%,  Validation Loss: 0.288\n",
      "Training Epoch 319 --- Training Accuracy:  90.6%, Validation Accuracy:  84.4%,  Validation Loss: 0.361\n",
      "Training Epoch 320 --- Training Accuracy:  90.6%, Validation Accuracy: 100.0%,  Validation Loss: 0.136\n",
      "Training Epoch 321 --- Training Accuracy:  90.6%, Validation Accuracy:  96.9%,  Validation Loss: 0.361\n",
      "Training Epoch 322 --- Training Accuracy:  90.6%, Validation Accuracy:  96.9%,  Validation Loss: 0.165\n",
      "Training Epoch 323 --- Training Accuracy:  90.6%, Validation Accuracy:  93.8%,  Validation Loss: 0.375\n",
      "Training Epoch 324 --- Training Accuracy:  90.6%, Validation Accuracy:  96.9%,  Validation Loss: 0.179\n",
      "Training Epoch 325 --- Training Accuracy:  90.6%, Validation Accuracy:  96.9%,  Validation Loss: 0.206\n",
      "Training Epoch 326 --- Training Accuracy:  90.6%, Validation Accuracy:  90.6%,  Validation Loss: 0.345\n",
      "Training Epoch 327 --- Training Accuracy:  90.6%, Validation Accuracy:  90.6%,  Validation Loss: 0.260\n",
      "Training Epoch 328 --- Training Accuracy:  90.6%, Validation Accuracy:  93.8%,  Validation Loss: 0.276\n",
      "Training Epoch 329 --- Training Accuracy:  90.6%, Validation Accuracy:  84.4%,  Validation Loss: 0.346\n",
      "Training Epoch 330 --- Training Accuracy:  90.6%, Validation Accuracy: 100.0%,  Validation Loss: 0.126\n",
      "Training Epoch 331 --- Training Accuracy:  90.6%, Validation Accuracy:  96.9%,  Validation Loss: 0.356\n",
      "Training Epoch 332 --- Training Accuracy:  90.6%, Validation Accuracy:  96.9%,  Validation Loss: 0.151\n",
      "Training Epoch 333 --- Training Accuracy:  90.6%, Validation Accuracy:  93.8%,  Validation Loss: 0.369\n",
      "Training Epoch 334 --- Training Accuracy:  90.6%, Validation Accuracy:  96.9%,  Validation Loss: 0.168\n",
      "Training Epoch 335 --- Training Accuracy:  90.6%, Validation Accuracy:  96.9%,  Validation Loss: 0.201\n",
      "Training Epoch 336 --- Training Accuracy:  90.6%, Validation Accuracy:  90.6%,  Validation Loss: 0.336\n",
      "Training Epoch 337 --- Training Accuracy:  90.6%, Validation Accuracy:  90.6%,  Validation Loss: 0.248\n",
      "Training Epoch 338 --- Training Accuracy:  90.6%, Validation Accuracy:  93.8%,  Validation Loss: 0.263\n",
      "Training Epoch 339 --- Training Accuracy:  90.6%, Validation Accuracy:  84.4%,  Validation Loss: 0.332\n",
      "Training Epoch 340 --- Training Accuracy:  90.6%, Validation Accuracy: 100.0%,  Validation Loss: 0.116\n",
      "Training Epoch 341 --- Training Accuracy:  90.6%, Validation Accuracy:  96.9%,  Validation Loss: 0.350\n",
      "Training Epoch 342 --- Training Accuracy:  90.6%, Validation Accuracy:  96.9%,  Validation Loss: 0.138\n",
      "Training Epoch 343 --- Training Accuracy:  90.6%, Validation Accuracy:  93.8%,  Validation Loss: 0.362\n",
      "Training Epoch 344 --- Training Accuracy:  90.6%, Validation Accuracy:  96.9%,  Validation Loss: 0.159\n",
      "Training Epoch 345 --- Training Accuracy:  90.6%, Validation Accuracy:  96.9%,  Validation Loss: 0.195\n",
      "Training Epoch 346 --- Training Accuracy:  90.6%, Validation Accuracy:  90.6%,  Validation Loss: 0.329\n",
      "Training Epoch 347 --- Training Accuracy:  90.6%, Validation Accuracy:  90.6%,  Validation Loss: 0.236\n",
      "Training Epoch 348 --- Training Accuracy:  90.6%, Validation Accuracy:  93.8%,  Validation Loss: 0.251\n",
      "Training Epoch 349 --- Training Accuracy:  90.6%, Validation Accuracy:  84.4%,  Validation Loss: 0.319\n",
      "Training Epoch 350 --- Training Accuracy:  90.6%, Validation Accuracy: 100.0%,  Validation Loss: 0.107\n",
      "Training Epoch 351 --- Training Accuracy:  90.6%, Validation Accuracy:  96.9%,  Validation Loss: 0.343\n",
      "Training Epoch 352 --- Training Accuracy:  90.6%, Validation Accuracy:  96.9%,  Validation Loss: 0.126\n",
      "Training Epoch 353 --- Training Accuracy:  90.6%, Validation Accuracy:  96.9%,  Validation Loss: 0.355\n",
      "Training Epoch 354 --- Training Accuracy:  90.6%, Validation Accuracy:  96.9%,  Validation Loss: 0.151\n",
      "Training Epoch 355 --- Training Accuracy:  90.6%, Validation Accuracy:  96.9%,  Validation Loss: 0.189\n",
      "Training Epoch 356 --- Training Accuracy:  90.6%, Validation Accuracy:  90.6%,  Validation Loss: 0.321\n",
      "Training Epoch 357 --- Training Accuracy:  90.6%, Validation Accuracy:  90.6%,  Validation Loss: 0.225\n",
      "Training Epoch 358 --- Training Accuracy:  90.6%, Validation Accuracy:  93.8%,  Validation Loss: 0.240\n",
      "Training Epoch 359 --- Training Accuracy:  90.6%, Validation Accuracy:  87.5%,  Validation Loss: 0.308\n",
      "Training Epoch 360 --- Training Accuracy:  90.6%, Validation Accuracy: 100.0%,  Validation Loss: 0.099\n",
      "Training Epoch 361 --- Training Accuracy:  90.6%, Validation Accuracy:  96.9%,  Validation Loss: 0.337\n",
      "Training Epoch 362 --- Training Accuracy:  90.6%, Validation Accuracy:  96.9%,  Validation Loss: 0.115\n",
      "Training Epoch 363 --- Training Accuracy:  90.6%, Validation Accuracy:  96.9%,  Validation Loss: 0.348\n",
      "Training Epoch 364 --- Training Accuracy:  90.6%, Validation Accuracy:  96.9%,  Validation Loss: 0.143\n",
      "Training Epoch 365 --- Training Accuracy:  90.6%, Validation Accuracy:  96.9%,  Validation Loss: 0.185\n",
      "Training Epoch 366 --- Training Accuracy:  90.6%, Validation Accuracy:  90.6%,  Validation Loss: 0.314\n",
      "Training Epoch 367 --- Training Accuracy:  90.6%, Validation Accuracy:  90.6%,  Validation Loss: 0.215\n",
      "Training Epoch 368 --- Training Accuracy:  90.6%, Validation Accuracy:  93.8%,  Validation Loss: 0.230\n",
      "Training Epoch 369 --- Training Accuracy:  90.6%, Validation Accuracy:  87.5%,  Validation Loss: 0.297\n",
      "Training Epoch 370 --- Training Accuracy:  90.6%, Validation Accuracy: 100.0%,  Validation Loss: 0.091\n",
      "Training Epoch 371 --- Training Accuracy:  90.6%, Validation Accuracy:  96.9%,  Validation Loss: 0.332\n",
      "Training Epoch 372 --- Training Accuracy:  90.6%, Validation Accuracy:  96.9%,  Validation Loss: 0.105\n",
      "Training Epoch 373 --- Training Accuracy:  90.6%, Validation Accuracy:  96.9%,  Validation Loss: 0.343\n",
      "Training Epoch 374 --- Training Accuracy:  90.6%, Validation Accuracy:  96.9%,  Validation Loss: 0.136\n",
      "Training Epoch 375 --- Training Accuracy:  90.6%, Validation Accuracy:  96.9%,  Validation Loss: 0.181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 376 --- Training Accuracy:  90.6%, Validation Accuracy:  87.5%,  Validation Loss: 0.307\n",
      "Training Epoch 377 --- Training Accuracy:  90.6%, Validation Accuracy:  90.6%,  Validation Loss: 0.204\n",
      "Training Epoch 378 --- Training Accuracy:  90.6%, Validation Accuracy:  93.8%,  Validation Loss: 0.220\n",
      "Training Epoch 379 --- Training Accuracy:  90.6%, Validation Accuracy:  87.5%,  Validation Loss: 0.286\n",
      "Training Epoch 380 --- Training Accuracy:  90.6%, Validation Accuracy: 100.0%,  Validation Loss: 0.084\n",
      "Training Epoch 381 --- Training Accuracy:  90.6%, Validation Accuracy:  96.9%,  Validation Loss: 0.327\n",
      "Training Epoch 382 --- Training Accuracy:  90.6%, Validation Accuracy:  96.9%,  Validation Loss: 0.096\n",
      "Training Epoch 383 --- Training Accuracy:  90.6%, Validation Accuracy:  96.9%,  Validation Loss: 0.338\n",
      "Training Epoch 384 --- Training Accuracy:  90.6%, Validation Accuracy:  96.9%,  Validation Loss: 0.129\n",
      "Training Epoch 385 --- Training Accuracy:  90.6%, Validation Accuracy:  96.9%,  Validation Loss: 0.178\n",
      "Training Epoch 386 --- Training Accuracy:  90.6%, Validation Accuracy:  87.5%,  Validation Loss: 0.301\n",
      "Training Epoch 387 --- Training Accuracy:  90.6%, Validation Accuracy:  90.6%,  Validation Loss: 0.194\n",
      "Training Epoch 388 --- Training Accuracy:  90.6%, Validation Accuracy:  93.8%,  Validation Loss: 0.212\n",
      "Training Epoch 389 --- Training Accuracy:  90.6%, Validation Accuracy:  87.5%,  Validation Loss: 0.275\n",
      "Training Epoch 390 --- Training Accuracy:  90.6%, Validation Accuracy: 100.0%,  Validation Loss: 0.078\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp/ipykernel_29288/239957495.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[0mtotal_iterations\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_iteration\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m \u001b[1;31m#train(num_iteration=100000)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp/ipykernel_29288/239957495.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(num_iteration)\u001b[0m\n\u001b[0;32m    187\u001b[0m                               y_true: y_valid_batch}\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m         \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict_tr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_examples\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\AI4AVenv\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 956\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    957\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\AI4AVenv\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1180\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1181\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\AI4AVenv\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1359\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\AI4AVenv\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1363\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\AI4AVenv\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1350\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\AI4AVenv\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1443\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import the necessary files and libraries that are required for operationalisation of this Project\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Randomise the seed for usage latero n\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "batch_size = 32\n",
    "\n",
    "# Prepare input data by obtaining the directory of the 8 image classes\n",
    "classes = os.listdir('training_data')\n",
    "num_classes = len(classes)\n",
    "\n",
    "# 15% of the data will automatically be used for validation\n",
    "validation_size = 0.15\n",
    "img_size = 128\n",
    "num_channels = 3\n",
    "train_path='training_data'\n",
    "\n",
    "# Load all the training and validation data into memory \n",
    "data = read_train_sets(train_path, img_size, classes, validation_size=validation_size)\n",
    "\n",
    "# Create a tensor flow session and label in preparation for training\n",
    "session = tf.Session()\n",
    "x = tf.placeholder(tf.float32, shape=[None, img_size,img_size,num_channels], name='x')\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true')\n",
    "y_true_cls = tf.argmax(y_true, dimension=1)\n",
    "\n",
    "\n",
    "\n",
    "# Set up three convolution layers\n",
    "filter_size_conv1 = 3 \n",
    "num_filters_conv1 = 32\n",
    "\n",
    "filter_size_conv2 = 3\n",
    "num_filters_conv2 = 32\n",
    "\n",
    "filter_size_conv3 = 3\n",
    "num_filters_conv3 = 64\n",
    "\n",
    "fc_layer_size = 128\n",
    "\n",
    "def create_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n",
    "\n",
    "def create_biases(size):\n",
    "    return tf.Variable(tf.constant(0.05, shape=[size]))\n",
    "\n",
    "def create_convolutional_layer(input,\n",
    "               num_input_channels, \n",
    "               conv_filter_size,        \n",
    "               num_filters):  \n",
    "    \n",
    "    ## We shall define the weights that will be trained using create_weights function.\n",
    "    weights = create_weights(shape=[conv_filter_size, conv_filter_size, num_input_channels, num_filters])\n",
    "    ## We create biases using the create_biases function. These are also trained.\n",
    "    biases = create_biases(num_filters)\n",
    "\n",
    "    ## Creating the convolutional layer\n",
    "    layer = tf.nn.conv2d(input=input, filter=weights,strides=[1, 1, 1, 1],padding='SAME')\n",
    "    layer += biases\n",
    "    layer = tf.nn.max_pool(value=layer,ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],padding='SAME')\n",
    "    layer = tf.nn.relu(layer)\n",
    "\n",
    "    return layer\n",
    "\n",
    "    \n",
    "# Function to flatten the shape of the data for returning an output value\n",
    "def create_flatten_layer(layer):\n",
    "    layer_shape = layer.get_shape()\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    layer = tf.reshape(layer, [-1, num_features])\n",
    "\n",
    "    return layer\n",
    "\n",
    "# Function to fully connect the shape of the data for returning an output value\n",
    "def create_fc_layer(input,          \n",
    "             num_inputs,    \n",
    "             num_outputs,\n",
    "             use_relu=True):\n",
    "    \n",
    "    # Define trainable weights and biases.\n",
    "    weights = create_weights(shape=[num_inputs, num_outputs])\n",
    "    biases = create_biases(num_outputs)\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "\n",
    "    return layer\n",
    "\n",
    "# Design the three convolution layers\n",
    "layer_conv1 = create_convolutional_layer(input=x,\n",
    "               num_input_channels=num_channels,\n",
    "               conv_filter_size=filter_size_conv1,\n",
    "               num_filters=num_filters_conv1)\n",
    "\n",
    "layer_conv2 = create_convolutional_layer(input=layer_conv1,\n",
    "               num_input_channels=num_filters_conv1,\n",
    "               conv_filter_size=filter_size_conv2,\n",
    "               num_filters=num_filters_conv2)\n",
    "\n",
    "layer_conv3= create_convolutional_layer(input=layer_conv2,\n",
    "               num_input_channels=num_filters_conv2,\n",
    "               conv_filter_size=filter_size_conv3,\n",
    "               num_filters=num_filters_conv3)\n",
    "          \n",
    "layer_flat = create_flatten_layer(layer_conv3)\n",
    "\n",
    "# Design the three fully connected layers\n",
    "layer_fc1 = create_fc_layer(input=layer_flat,\n",
    "                     num_inputs=layer_flat.get_shape()[1:4].num_elements(),\n",
    "                     num_outputs=fc_layer_size,\n",
    "                     use_relu=True)\n",
    "\n",
    "layer_fc2 = create_fc_layer(input=layer_fc1,\n",
    "                     num_inputs=fc_layer_size,\n",
    "                     num_outputs=num_classes,\n",
    "                     use_relu=False) \n",
    "\n",
    "# Create a predicted dataset\n",
    "y_pred = tf.nn.softmax(layer_fc2,name='y_pred')\n",
    "y_pred_cls = tf.argmax(y_pred, dimension=1)\n",
    "session.run(tf.global_variables_initializer())\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=layer_fc2, labels=y_true)\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-6).minimize(cost)\n",
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "session.run(tf.global_variables_initializer()) \n",
    "\n",
    "# Function to show the progress of the training\n",
    "def show_progress(epoch, feed_dict_train, feed_dict_validate, val_loss):\n",
    "    acc = session.run(accuracy, feed_dict=feed_dict_train)\n",
    "    val_acc = session.run(accuracy, feed_dict=feed_dict_validate)\n",
    "    msg = \"Training Epoch {0} --- Training Accuracy: {1:>6.1%}, Validation Accuracy: {2:>6.1%},  Validation Loss: {3:.3f}\"\n",
    "    print(msg.format(epoch + 1, acc, val_acc, val_loss))\n",
    "\n",
    "total_iterations = 0\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Function to support the actual training of the model for 1,000,000 iterations using a split train/test split\n",
    "def train(num_iteration):\n",
    "    global total_iterations\n",
    "    \n",
    "    for i in range(total_iterations,\n",
    "                   total_iterations + num_iteration):\n",
    "\n",
    "        x_batch, y_true_batch, _, cls_batch = data.train.next_batch(batch_size)\n",
    "        x_valid_batch, y_valid_batch, _, valid_cls_batch = data.valid.next_batch(batch_size)\n",
    "\n",
    "        \n",
    "        feed_dict_tr = {x: x_batch,\n",
    "                           y_true: y_true_batch}\n",
    "        feed_dict_val = {x: x_valid_batch,\n",
    "                              y_true: y_valid_batch}\n",
    "\n",
    "        session.run(optimizer, feed_dict=feed_dict_tr)\n",
    "\n",
    "        if i % int(data.train.num_examples/batch_size) == 0: \n",
    "            val_loss = session.run(cost, feed_dict=feed_dict_val)\n",
    "            epoch = int(i / int(data.train.num_examples/batch_size))    \n",
    "            \n",
    "            show_progress(epoch, feed_dict_tr, feed_dict_val, val_loss)\n",
    "            saver.save(session, './roadsurface-model') \n",
    "\n",
    "\n",
    "    total_iterations += num_iteration\n",
    "\n",
    "train(num_iteration=1000000)    \n",
    "\n",
    "print('\\a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0bf9d733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./roadsurface-model\n",
      "[0.00086166855, 0.05278209, 0.21910238, 2.7362074e-05, 0.7125144, 0.002579258, 2.2048416e-06, 0.012130604]\n",
      "Classification done!\n",
      "Results saved as:  imagery.avi\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary files and libraries that are required for operationalisation of this Project\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import sys\n",
    "import os.path\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "import operator\n",
    "\n",
    "image_size=128\n",
    "num_channels=3\n",
    "images = []\n",
    "\n",
    "# Set up details for producing the output file to be tested\n",
    "outputFile = \"imagery.avi\" \n",
    "inputfile = r\".\\images\\3.jpg\"\n",
    "# Opening frames to be created from the designated output file\n",
    "cap = cv.VideoCapture(inputfile)\n",
    "vid_writer = cv.VideoWriter(outputFile, cv.VideoWriter_fourcc('M','J','P','G'), 15, (round(cap.get(cv.CAP_PROP_FRAME_WIDTH)),round(cap.get(cv.CAP_PROP_FRAME_HEIGHT))))\n",
    "\n",
    "width = int(round(cap.get(cv.CAP_PROP_FRAME_WIDTH)))\n",
    "height = int(round(cap.get(cv.CAP_PROP_FRAME_HEIGHT)))\n",
    "newHeight = int(round(height/2))\n",
    "\n",
    "# Restoring the model that was trained and created as part of the previous section\n",
    "sess = tf.Session()\n",
    "saver = tf.train.import_meta_graph('roadsurface-model.meta')\n",
    "saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "\n",
    "# Acessing the graph of the model created\n",
    "graph = tf.get_default_graph()\n",
    "y_pred = graph.get_tensor_by_name(\"y_pred:0\")\n",
    "x = graph.get_tensor_by_name(\"x:0\")\n",
    "y_true = graph.get_tensor_by_name(\"y_true:0\")\n",
    "y_test_images = np.zeros((1, len(os.listdir('training_data'))))\n",
    "\n",
    "# Compile and classify images with the ranking of the state of the road surface quality\n",
    "while cv.waitKey(1) < 0:\n",
    "    hasFrame, images = cap.read()\n",
    "    finalimg = images\n",
    "\n",
    "    if not hasFrame:\n",
    "        print(\"Classification done!\")\n",
    "        print(\"Results saved as: \", outputFile)\n",
    "        cv.waitKey(3000)\n",
    "        break\n",
    "\n",
    "    images = images[newHeight-5:height-50, 0:width]\n",
    "    images = cv.resize(images, (image_size, image_size), 0, 0, cv.INTER_LINEAR)\n",
    "    images = np.array(images, dtype=np.uint8)\n",
    "    images = images.astype('float32')\n",
    "    images = np.multiply(images, 1.0/255.0)\n",
    "\n",
    "    x_batch = images.reshape(1, image_size, image_size, num_channels)\n",
    "    feed_dict_testing = {x: x_batch, y_true: y_test_images}\n",
    "    result = sess.run(y_pred, feed_dict=feed_dict_testing)\n",
    "\n",
    "    # Determine the alignment to each of the classes\n",
    "    outputs = [result[0,0], result[0,1], result[0,2], result[0,3], result[0,4], result[0,5], result[0,6], result[0,7]]\n",
    "\n",
    "    # Pick the class that has the best alignment as it'll refer to the end output and preferred classification\n",
    "    value = max(outputs)\n",
    "    index = np.argmax(outputs)\n",
    "\n",
    "    # Define mapping for output/index to the output as to the road surface quality\n",
    "    perfectColor = (255,0,255) #Perfect\n",
    "    driveableColor = (0,255,0) #Driveable\n",
    "    cautionColor = (0,255,255) #Caution\n",
    "    damagedColor = (0,90,255) #Damaged\n",
    "    undriveableColor = (0,0,255) #Damaged\n",
    "    \n",
    "    if index == 0:\n",
    "        #label = 'Tarmac - Good'\n",
    "        label = 'Perfect'\n",
    "        prob = str(\"{0:.2f}\".format(value))\n",
    "        color = perfectColor\n",
    "    elif index == 1:\n",
    "        #label = 'Tarmac - Regular'\n",
    "        label = 'Driveable'\n",
    "        prob = str(\"{0:.2f}\".format(value))\n",
    "        color = driveableColor\n",
    "    elif index == 2:\n",
    "        #label = 'Tarmac - Bad'\n",
    "        label = 'Caution'\n",
    "        prob = str(\"{0:.2f}\".format(value))\n",
    "        color = cautionColor\n",
    "    elif index == 3:\n",
    "        #label = 'Paved - Good'\n",
    "        label = 'Driveable'\n",
    "        prob = str(\"{0:.2f}\".format(value))\n",
    "        color = driveableColor\n",
    "    elif index == 4:\n",
    "        #label = 'Paved - Regular'\n",
    "        label = 'Driveable'\n",
    "        prob = str(\"{0:.2f}\".format(value))\n",
    "        color = driveableColor\n",
    "    elif index == 5:\n",
    "        #label = 'Paved - Bad'\n",
    "        label = 'Caution'\n",
    "        prob = str(\"{0:.2f}\".format(value))\n",
    "        color = cautionColor\n",
    "    elif index == 6:\n",
    "        #label = 'Dirt - Regular'\n",
    "        label = 'Damaged'\n",
    "        prob = str(\"{0:.2f}\".format(value))\n",
    "        color = damagedColor\n",
    "    elif index == 7:\n",
    "        #label = 'Dirt - Bad'\n",
    "        label = 'Undriveable'\n",
    "        prob = str(\"{0:.2f}\".format(value))\n",
    "        color = undriveableColor \n",
    "        \n",
    "    # Draw the rectangle of values highlighting whether the image dictates a good or bad road surface\n",
    "    cv.rectangle(finalimg, (0, 0), (300, 40), (255, 255, 255), cv.FILLED)\n",
    "    cv.putText(finalimg, 'Road Condition: ', (5,15), cv.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1)\n",
    "    cv.putText(finalimg, label, (150,15), cv.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "\n",
    "    vid_writer.write(finalimg.astype(np.uint8))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5b216b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431c8ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
